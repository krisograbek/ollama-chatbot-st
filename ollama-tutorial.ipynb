{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore how to use open-source models with **Ollama**.\n",
    "\n",
    "**Ollama** is a convenient platform for local development of open-source AI models.\n",
    "Before Ollama, it used to be complicated to run open-source LLMs locally. It used to be very technical and required good understanding of computer hardware and architecture.\n",
    "\n",
    "With Ollama, running local models is straightforward.\n",
    "\n",
    "Here's all you need:\n",
    "1. [Download Ollama](https://ollama.com/download) on you local system.\n",
    "2. Download one of the local models on your computer using Ollama.\n",
    "For example, if I want to use Llama3, I need to open the terminal and run:\n",
    "```bash\n",
    "$ ollama run llama3\n",
    "```\n",
    "\n",
    "If it's the first time I use the model, Ollama will first download it. Because it has 8B parameters, it'll take a while.\n",
    "\n",
    "Once the model is downloaded, we can also use it through Ollama API.\n",
    "\n",
    "To install Ollama API, run the following command:\n",
    "```bash\n",
    "$ pip install ollama\n",
    "```\n",
    "\n",
    "And with these steps, you're ready to run the code from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Response\n",
    "\n",
    "Now it's time to test our model. Let's just ask a simple question to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "Here's all we did:\n",
    "- `import ollama` to use Ollama API\n",
    "- `model = \"llama3` to define the model we want to use\n",
    "- `ollama.chat()` to get the response. We used 2 parameters:\n",
    "    1. `model` that we defined before\n",
    "    2. `messages` where we keep the list of messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Let's play with some LLM parameters:\n",
    "1. System prompt.\n",
    "2. Temperature.\n",
    "3. Max tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While there's no one-size-fits-all formula for getting rich, here are some general tips that\n"
     ]
    }
   ],
   "source": [
    "def generate_response(messages, **kwargs):\n",
    "    response = ollama.chat(model=model, messages=messages, **kwargs)\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"How to get rich?\"}]\n",
    "response = generate_response(messages, options={\"num_predict\": 20})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting rich requires a combination of smart financial decisions, hard work, and a bit of luck. Here\n"
     ]
    }
   ],
   "source": [
    "ollama.chat(model=model, messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "A nice feature of Ollama is the ability to stream responses. Afrer using ChatGPT or Claude, we expect the responses to run as streams. Here's how to do it.\n",
    "\n",
    "The biggest change will come from the `stream` parameter. We just set it to `True`. \n",
    "\n",
    "But we also need to run the `ollama.chat()` in a for loop.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      " capital\n",
      " of\n",
      " Poland\n",
      " is\n",
      " Warsaw\n",
      " (\n",
      "Pol\n",
      "ish\n",
      ":\n",
      " Wars\n",
      "z\n",
      "awa\n",
      ").\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}]\n",
    "\n",
    "for chunk in ollama.chat(model=model, messages=messages, stream=True):\n",
    "    token = chunk[\"message\"][\"content\"]\n",
    "    if token is not None:\n",
    "        print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
