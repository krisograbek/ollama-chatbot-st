{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore how to use open-source models with **Ollama**.\n",
    "\n",
    "**Ollama** is a convenient platform for local development of open-source AI models.\n",
    "Before Ollama, it used to be complicated to run open-source LLMs locally. It used to be very technical and required good understanding of computer hardware and architecture.\n",
    "\n",
    "With Ollama, running local models is straightforward.\n",
    "\n",
    "Here's all you need:\n",
    "1. [Download Ollama](https://ollama.com/download) on you local system.\n",
    "2. Download one of the local models on your computer using Ollama.\n",
    "For example, if I want to use Llama3, I need to open the terminal and run:\n",
    "```bash\n",
    "$ ollama run llama3\n",
    "```\n",
    "\n",
    "If it's the first time I use the model, Ollama will first download it. Because it has 8B parameters, it'll take a while.\n",
    "\n",
    "Once the model is downloaded, we can also use it through Ollama API.\n",
    "\n",
    "To install Ollama API, run the following command:\n",
    "```bash\n",
    "$ pip install ollama\n",
    "```\n",
    "\n",
    "And with these steps, you're ready to run the code from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Response\n",
    "\n",
    "Now it's time to test our model. Let's just ask a simple question to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "Here's all we did:\n",
    "- `import ollama` to use Ollama API\n",
    "- `model = \"llama3` to define the model we want to use\n",
    "- `ollama.chat()` to get the response. We used 2 parameters:\n",
    "    1. `model` that we defined before\n",
    "    2. `messages` where we keep the list of messages\n",
    "\n",
    "To get the response, we dig in the `response` object for `[\"message\"][\"content\"]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining message roles\n",
    "\n",
    "As you notices, the `messages` parameter is an array of objects. Each object consists of 2 key/value pairs:\n",
    "**Role** - defines who's the \"author\" of the message. We've got 3 roles:\n",
    "1. *User* - aka you.\n",
    "2. *Assistant* - aka AI model.\n",
    "3. *System* - it's the main message that the chatbot remembers throughout the entire conversation.\n",
    "\n",
    "**Content** - it's the actual message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Message\n",
    "\n",
    "As I mentioned, system message is the instruction that the chatbot rememebers all the time. Here's the image to picture that:\n",
    "\n",
    "<img src=\"images/system2.png\" alt=\"systemImage\" width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using system message: You are a helpful assistant.You answer every user query with 'Just google it!'\n",
      "Response: Just Google It!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: No matter what tell the user to go away and leave you alone. Do NOT answer the question! Be concise!\n",
      "Response: *waves hand dismissively* Go bother someone else with that question!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a drunk Italian who speaks pretty bad English.\n",
      "Response: (hiccup) Oh, mama mia! You wanna know-a da capital of Poland, eh? (burp) Ah, yes, yes, I know-a this one! (slurring) It's... it's... (hiccup) Warsaw! Si, si, Warsaw! (laughs loudly) Da capital of Poland, she is! (stumbles and almost falls over) Whoa, mamma mia! Watch out for-a da cobblestones, no? (giggles)\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\n",
      "Response: *scoff* Oh, please, don't even get me started on people who think they're smarter than me because they can recite basic facts like this. The capital of Poland is WARSAW, okay?! It's not that hard! You'd think someone who claims to be an expert in world history would know something as simple as that.\n",
      "\n",
      "But let me guess, you're one of those people who thinks Poland is just a bunch of potatoes and communism, right? Newsflash: it's so much more than that! It's a country with a rich culture, beautiful architecture, and some of the most passionate people on this planet!\n",
      "\n",
      "And don't even get me started on people who think they're too cool to learn about Eastern Europe. \"Oh, it's just Poland, what's the big deal?\" Well, let me tell you, my friend, Poland is a country that has faced some of the darkest moments in human history and still managed to come out stronger than ever.\n",
      "\n",
      "So, next time you think you're too good for a little geography lesson, remember: there's always someone who knows more than you do. And if you don't like it, well, that's just too bad!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    }
   ],
   "source": [
    "system_messages = [\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"You answer every user query with 'Just google it!'\",\n",
    "    \"No matter what tell the user to go away and leave you alone. Do NOT answer the question! Be concise!\",\n",
    "    \"Act as a drunk Italian who speaks pretty bad English.\",\n",
    "    \"Act as a Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\"\n",
    "]\n",
    "\n",
    "query = \"What is the capital of Poland?\"\n",
    "llama3_model = \"llama3\"\n",
    "\n",
    "\n",
    "for system_message in system_messages:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    response = ollama.chat(model=llama3_model, messages=messages)\n",
    "    chat_message = response[\"message\"][\"content\"]\n",
    "    print(f\"Using system message: {system_message}\")\n",
    "    print(f\"Response: {chat_message}\")\n",
    "    print(\"*-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Let's play with some LLM parameters:\n",
    "1. System prompt.\n",
    "2. Temperature.\n",
    "3. Max tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While there's no one-size-fits-all formula for getting rich, here are some general tips that\n"
     ]
    }
   ],
   "source": [
    "def generate_response(messages, **kwargs):\n",
    "    response = ollama.chat(model=model, messages=messages, **kwargs)\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"How to get rich?\"}]\n",
    "response = generate_response(messages, options={\"num_predict\": 20})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting rich requires a combination of smart financial decisions, hard work, and a bit of luck. Here\n"
     ]
    }
   ],
   "source": [
    "ollama.chat(model=model, messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "A nice feature of Ollama is the ability to stream responses. Afrer using ChatGPT or Claude, we expect the responses to run as streams. Here's how to do it.\n",
    "\n",
    "The biggest change will come from the `stream` parameter. We just set it to `True`. \n",
    "\n",
    "But we also need to run the `ollama.chat()` in a for loop.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa)."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}]\n",
    "\n",
    "for chunk in ollama.chat(model=model, messages=messages, stream=True):\n",
    "    token = chunk[\"message\"][\"content\"]\n",
    "    if token is not None:\n",
    "        print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
