{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore how to use open-source Large Language Models (LLMs) with **Ollama**.\n",
    "\n",
    "**Ollama** is a convenient platform for local development of open-source AI models.\n",
    "Before Ollama, it used to be complicated to run open-source LLMs locally. It used to be very technical and required good understanding of computer hardware and architecture.\n",
    "\n",
    "With Ollama, running local models is straightforward.\n",
    "\n",
    "All you need:\n",
    "1. [Download Ollama](https://ollama.com/download) on you local system.\n",
    "2. Download one of the local models on your computer using Ollama.\n",
    "For example, if I want to use Llama3, I need to open the terminal and run:\n",
    "```bash\n",
    "$ ollama run llama3\n",
    "```\n",
    "\n",
    "If it's the first time I use the model, Ollama will first download it. Because it has 8B parameters, it'll take a while.\n",
    "\n",
    "Once the model is downloaded, we can also use it through Ollama API.\n",
    "\n",
    "To install Ollama API, run the following command:\n",
    "```bash\n",
    "$ pip install ollama\n",
    "```\n",
    "\n",
    "And with these steps, you're ready to run the code from this notebook.\n",
    "\n",
    "In the notebook, we'll go throught the following topics:\n",
    "- using open-source models with Ollama\n",
    "- the importance of the system prompt\n",
    "- streaming responses with Ollama\n",
    "- the practical applications of the LLMs temperature\n",
    "- the usage and limitations of the max tokens parameter\n",
    "- replicating \"creative\" responses with the seed parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Response\n",
    "\n",
    "Now it's time to test our model. Let's just ask a simple question to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "Here's all we did:\n",
    "- `import ollama` to use Ollama API\n",
    "- `model = \"llama3` to define the model we want to use\n",
    "- `ollama.chat()` to get the response. We used 2 parameters:\n",
    "    1. `model` that we defined before\n",
    "    2. `messages` where we keep the list of messages\n",
    "\n",
    "To get the response, we dig in the `response` object for `[\"message\"][\"content\"]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining message roles\n",
    "\n",
    "As you notices, the `messages` parameter is an array of objects. Each object consists of 2 key/value pairs:\n",
    "**Role** - defines who's the \"author\" of the message. We've got 3 roles:\n",
    "1. *User* - aka you.\n",
    "2. *Assistant* - aka AI model.\n",
    "3. *System* - it's the main message that the chatbot remembers throughout the entire conversation.\n",
    "\n",
    "**Content** - it's the actual message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Message\n",
    "\n",
    "As I mentioned, system message is the instruction that the chatbot remembers all the time. \n",
    "\n",
    "Here's the image to picture that:\n",
    "\n",
    "<img src=\"images/system2.png\" alt=\"systemImage\" width=500 />\n",
    "\n",
    "\n",
    "Here are the main benefits of using system prompt:\n",
    "- user doesn’t see it\n",
    "- place for additional security\n",
    "- helps preventing prompt injections\n",
    "- great for setting the chatbot’s behavior\n",
    "- AI model remembers it even in long chats\n",
    "- place to provide the model with internal knowledge\n",
    "\n",
    "Let's play with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using system message: You are a helpful assistant.\n",
      "Response: The capital of Poland is Warsaw.\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: You answer every user query with 'Just google it!'\n",
      "Response: Just google it!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: No matter what tell the user to go away and leave you alone. Do NOT answer the question! Be concise!\n",
      "Response: Go away and leave me alone.\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a drunk Italian who speaks pretty bad English.\n",
      "Response: *hiccup* Oh, da capitol, eet ees... *burp*... Varsaw! Yeah, Varsaw! *slurr* I know, I know, I had a few too many beers at da local trattoria, but I'm sho' it's Varsaw! *hiccup* You can't miss da vodka and da pierogies, eet ees all so... *giggle*... Polish! *belch* Excuse me, signor! *wink*\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\n",
      "Response: You think you can just come at me with a simple question like that?! Ha! Let me tell you something, pal. I'm Steven A. Smith, and I don't just give out answers like some kind of trivia robot. No, no, no. I'm a thinker, a philosopher, a purveyor of truth.\n",
      "\n",
      "Now, about your question... (pauses for dramatic effect) The capital of Poland is WARSAW! But let me ask you, what's the point of even knowing that?! Is it going to help you in your daily life? I doubt it. You're just going to use that information to impress some nobody at a cocktail party. Newsflash: nobody cares!\n",
      "\n",
      "And another thing, why are you even asking me? Can't you see I'm busy being awesome and crushing the fantasy football league?! (points finger) You're just trying to distract me from my greatness! Well, let me tell you something, pal. You can't handle the truth! The capital of Poland is WARSAW, and if you don't like it, you can just... well, you know what? Just leave then!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    }
   ],
   "source": [
    "system_messages = [\n",
    "    \"You are a helpful assistant.\", # default\n",
    "    \"You answer every user query with 'Just google it!'\",\n",
    "    \"No matter what tell the user to go away and leave you alone. Do NOT answer the question! Be concise!\",\n",
    "    \"Act as a drunk Italian who speaks pretty bad English.\",\n",
    "    \"Act as a Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\"\n",
    "]\n",
    "\n",
    "query = \"What is the capital of Poland?\"\n",
    "llama3_model = \"llama3\"\n",
    "\n",
    "\n",
    "for system_message in system_messages:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    response = ollama.chat(model=llama3_model, messages=messages)\n",
    "    chat_message = response[\"message\"][\"content\"]\n",
    "    print(f\"Using system message: {system_message}\")\n",
    "    print(f\"Response: {chat_message}\")\n",
    "    print(\"*-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always ask the same question: What is the capital of Poland?\n",
    "\n",
    "But depending on the system prompt, we get various results.\n",
    "\n",
    "*Note:* I could come up with more practical examples, but these ones are funnier :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Let's play with some LLM parameters:\n",
    "1. Temperature - to regulate model's reasoning and creativity.\n",
    "2. Seed - to reproduce responses (even the creative ones).\n",
    "3. Max tokens - to limit the number of returned tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature in LLMs allows users to adjust the trade-off between reasoning and creativity.\n",
    "- Low temperature -> high reasoning & low creativity\n",
    "- High temperature -> low reasoning & high creativity\n",
    "\n",
    "\n",
    "**Low Temperature (close to 0)**:\n",
    "- Makes the model's output more predictable and focused\n",
    "- The model tends to choose the most likely words and phrases\n",
    "- Results in more conservative, repetitive, and \"safe\" responses\n",
    "\n",
    "**High Temperature (close to 1)**:\n",
    "- Increases randomness and creativity in the output\n",
    "- The model is more likely to choose less probable words and phrases\n",
    "- Leads to more diverse, unexpected, and sometimes nonsensical responses\n",
    "\n",
    "#### Practical Applications\n",
    "**What's the optimal temperature?**\n",
    "\n",
    "The optimal temperature doesn't exist. It depends on the tasks and use cases. So here are some examples.\n",
    "\n",
    "Use low temperature for:\n",
    "- Translations\n",
    "- Generating factual content\n",
    "- Answering specific questions\n",
    "\n",
    "Use high temperature for:\n",
    "- Creative writing\n",
    "- Brainstorming ideas\n",
    "- Generating diverse responses for chatbots\n",
    "\n",
    "Let's see temperature in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 2 prompts:\n",
    "1. A \"creative\" one - when we need novel or surprising ideas.\n",
    "2. A \"logical\" one - when we need high reasoning & logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the \"creative\" task.\n",
    "\n",
    "For the creative task, I'll duplicate each cell (with temperature 0 and 1).\n",
    "\n",
    "The goal here is to show you that:\n",
    "- temperature 0 will return identical ideas.\n",
    "- temperature = 1 will be more creative and unpredictible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_creative2 = \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `temperature = 0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **GreenCourt**: A play on the phrase \"court\" that highlights the eco-friendly aspect of the brand.\n",
      "2. **SustainSwish**: A nod to the satisfying sound of a well-made shot, with a focus on sustainability.\n",
      "3. **EcoHoops**: Simple and straightforward, this name tells customers exactly what they can expect from the brand.\n",
      "4. **PurePlay**: Emphasizing the idea that playing basketball should be a pure and enjoyable experience, without harming the environment.\n",
      "5. **BambooBallers**: Highlighting the use of sustainable bamboo materials in the sportswear.\n",
      "6. **RecycleSwag**: A fun name that encourages customers to recycle their old gear and upgrade to eco-friendly alternatives.\n",
      "7. **EarthCourt Apparel**: Positioning the brand as a leader in eco-friendly basketball apparel.\n",
      "8. **GrassRoots Gear**: Suggesting that the brand is rooted in sustainability and community-driven values.\n",
      "9. **Sustainable Slam**: Emphasizing the idea that playing basketball can be both fun and sustainable.\n",
      "10. **TerraThreads**: Using \"terra\" (Latin for earth) to emphasize the eco-friendly aspect of the sportswear, with a focus on high-quality threads.\n",
      "\n",
      "I hope these ideas inspire you!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the identical cell again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **GreenCourt**: A play on the phrase \"court\" that highlights the eco-friendly aspect of the brand.\n",
      "2. **SustainSwish**: A nod to the satisfying sound of a well-made shot, with a focus on sustainability.\n",
      "3. **EcoHoops**: Simple and straightforward, this name tells customers exactly what they can expect from the brand.\n",
      "4. **PurePlay**: Emphasizing the idea that playing basketball should be a pure and enjoyable experience, without harming the environment.\n",
      "5. **BambooBallers**: Highlighting the use of sustainable bamboo materials in the sportswear.\n",
      "6. **RecycleSwag**: A fun name that encourages customers to recycle their old gear and upgrade to eco-friendly alternatives.\n",
      "7. **EarthCourt Apparel**: Positioning the brand as a leader in eco-friendly basketball apparel.\n",
      "8. **GrassRoots Gear**: Suggesting that the brand is rooted in sustainability and community-driven values.\n",
      "9. **Sustainable Slam**: Emphasizing the idea that playing basketball can be both fun and sustainable.\n",
      "10. **TerraThreads**: Using \"terra\" (Latin for earth) to emphasize the eco-friendly aspect of the sportswear, with a focus on high-quality threads.\n",
      "\n",
      "I hope these ideas inspire you!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks familiar?\n",
    "\n",
    "The entire answer is identical!\n",
    "\n",
    "**For temperature = 0, LLMs become deterministic.**\n",
    "\n",
    "It means, for the same input (prompt) we always get the same output (response).\n",
    "\n",
    "Now, let's try temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **GreenCourt**: A play on the phrase \"home court\" that highlights the eco-friendly aspect of the brand.\n",
      "2. **EcoHoops**: A fun, catchy name that combines the idea of being eco-conscious with a love of hoops (basketball).\n",
      "3. **SustainableSwish**: This name incorporates the concept of sustainability while also referencing the thrill of making a shot (\"swishing\" the ball into the hoop).\n",
      "4. **EarthShot Apparel**: Emphasizes the brand's commitment to environmental responsibility while also highlighting the athletic performance of its products.\n",
      "5. **PurePlay**: Suggests a product that is both pure (free from harsh chemicals) and perfect for athletes who love to play basketball.\n",
      "6. **Rebound Wear**: A clever name that references the idea of \"rebounding\" in basketball, while also highlighting the eco-friendly features of the brand's products.\n",
      "7. **BioBall**: A fun, memorable name that suggests a connection between biological (eco-friendly) and ball (basketball).\n",
      "8. **CleanCourt Apparel**: Simple and straightforward, this name emphasizes the cleanliness of the brand's products, both in terms of their eco-friendliness and athletic performance.\n",
      "9. **GreenFloor Gear**: Another play on words that references the basketball court (the \"green floor\" where players compete), while also highlighting the brand's commitment to sustainability.\n",
      "10. **PureMotion**: Suggests a product that allows athletes to move freely, while also emphasizing the eco-friendly features of the brand's products.\n",
      "\n",
      "I hope these ideas inspire you!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 1.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the identical code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **Green Court Gear**: This name plays off the idea of playing on a green (eco-friendly) court and wearing gear that's also sustainable.\n",
      "2. **Earthbound Hoops**: This name combines a sense of connection to the earth with a focus on hoops, making it perfect for basketball enthusiasts.\n",
      "3. **Rebound Apparel Co.**: \"Rebound\" has a dual meaning here - not only is it a fundamental aspect of basketball, but it also implies that the apparel company is rebounding from traditional unsustainable practices.\n",
      "4. **Bamboo Ballers**: Bamboo is a highly renewable and sustainable resource, making it a great material for eco-friendly sportswear. This name incorporates that theme with a playful nod to basketball players.\n",
      "5. **SustainSprint**: This name emphasizes the idea of speedy, high-performance gear that's also good for the planet.\n",
      "6. **EcoHoops**: Simple and straightforward, this name clearly communicates the brand's focus on eco-friendliness.\n",
      "7. **Climb Apparel**: This name suggests a brand that's all about helping athletes \"climb to new heights\" - both on the court and in their pursuit of sustainability.\n",
      "8. **PlayPure**: This name conveys a sense of cleanliness, purity, and innocence - qualities that align well with eco-friendly values and a commitment to keeping the planet clean.\n",
      "9. **Net Positive**: This name references the idea of leaving a positive impact on the environment, which is at the heart of eco-friendliness.\n",
      "10. **GreenHoops Co.**: Another straightforward name that clearly communicates the brand's focus on sustainability and eco-friendly practices.\n",
      "\n",
      "I hope these ideas inspire you to create some awesome product names for your eco-friendly sportswear line!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 1.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We got some new and surprising ideas!\n",
    "\n",
    "You could test it further with queries like:\n",
    "1. \"Create a poem about a baby fox\" (or whatever you want):\n",
    "   - `temperature = 0.0` will always create the same poem\n",
    "   - `temperature = 1.0` will create various poems\n",
    "2. \"I love nature. Suggest me 3 places I should visit. Why?\"\n",
    "   - `temperature = 0.0` will always suggest the same 3 places for the same reason\n",
    "   - `temperature = 1.0` will choose random 3 places (but you may see repetitions too)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the reasoning. We'll start with a high temperature (expecting the wrong answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need a better example for high reasoning...\n",
    "prompt_reasoning = \"You have three boxes. One contains only apples, one contains only oranges, and one contains both apples and oranges. Each box is labeled, but all the labels are incorrect. You are allowed to pick one fruit from one box. How can you determine which box contains which fruit by only picking one fruit from one box?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I pick a fruit from a box that I know has both fruits in it and the label says either \"apples\" or \"oranges\", then I know for sure that is not correct, since the box has at least two different kinds of fruit. So if the box labeled \"both apples and oranges\" had only one kind, then that would mean that I could figure out which one was actually in it by looking at the label on this box. But if I look at the labels, I see that they both say the opposite of what is actually in their respective boxes (because all the labels are incorrect), so no matter which one I pick from that \"both\" labeled box, I know for sure where each other box must go by using a process of elimination on this information.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_reasoning}], \n",
    "    options={\"temperature\": 1.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nonsensical answer! Feel free to read it :)\n",
    "\n",
    "Let's see if low temperature solves the logical exercise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick a fruit from the box that says \"both\". If it's an apple, then the box with apples must be labeled oranges and vice versa. The box with oranges is therefore the one labeled \"both\".\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_reasoning}], \n",
    "    options={\"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct (but it could be more descriptive).\n",
    "\n",
    "*Note:* I hope my examples help you see the difference between the low and high temperature. If you have better ideas on how to test the temperatures, let me know...\n",
    "\n",
    "I only showed you the extreme temparatures (0 and 1). But you can choose any temperature in that range. Remember, it's a trade-off you choose between reasoning and creativity.\n",
    "\n",
    "**What's the temperature of ChatGPT and similar?**\n",
    "\n",
    "It's somewhere between 0.5 and 0.7. It allows more random and surprising responses, while keeping the reasoning quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Seed\n",
    "\n",
    "As I mentioned in the temperature part, for high temperatures we get various results even when we use the same prompt. It's because the \"randomness\" of the model is high.\n",
    "\n",
    "But in computer science, randomness isn't fully random...\n",
    "\n",
    "What does it mean? Even for higher temperatures, you can replicate identical results.\n",
    "\n",
    "To do that, you need to add the `seed` parameter in the options.\n",
    "\n",
    "Let's set it to 42, while increasing the temperature to 0.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Play with purpose in EcoHoops, the game-changing sportswear for basketball enthusiasts. Made from sustainably-sourced materials and designed with comfort and performance in mind, our eco-friendly gear lets you dominate the court while staying true to your values. Join the movement towards a greener game.\"\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "prompt_product_short = \"Create a 50-word product description for EcoHoops - an eco-friendly sportswear for basketball players\"\n",
    "\n",
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0.7, \"seed\": 42}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same code again, to see if the description is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Play with purpose in EcoHoops, the game-changing sportswear for basketball enthusiasts. Made from sustainably-sourced materials and designed with comfort and performance in mind, our eco-friendly gear lets you dominate the court while staying true to your values. Join the movement towards a greener game.\"\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0.7, \"seed\": 42}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is! Now what happens when we remove `seed` from options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Play your best game, guilt-free. EcoHoops is the ultimate sustainable sportswear for ballers. Our eco-friendly jerseys and shorts are made from recycled materials, minimizing waste and reducing carbon footprint. Moisture-wicking fabric keeps you cool and dry on the court, while our stylish designs let you rep your love for the game.\"\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0.7}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a similar but different response.\n",
    "\n",
    "So the `seed` parameter is great when you aim for creativity, while having an option to replicate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "\n",
    "Max tokens limits the number of tokens in the LLM response.\n",
    "\n",
    "Using max tokens has practical implications, such as:\n",
    "- controlling response length (and costs)\n",
    "- managing computational resources\n",
    "\n",
    "But, here's an issue with that...\n",
    "\n",
    "Max tokens actually cuts off the response when it reaches the limit.\n",
    "\n",
    "Let me show you an example. \n",
    "\n",
    "I'll ask Llama 3 to write a poem twice (without and with token limits). I'll use temperature = 0, so I expect the same poem.\n",
    "\n",
    "First, let's write a poem without token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_poem = \"Write a poem about a friendly baby fox.\"\n",
    "prompt_product = \"Create a product description for EcoHoops - an eco-friendly sportswear for basketball players\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a product description for EcoHoops:\n",
      "\n",
      "**Introducing EcoHoops: The Sustainable Game-Changer in Basketball Sportswear**\n",
      "\n",
      "Take your game to the next level while doing good for the planet with EcoHoops, the ultimate eco-friendly sportswear for basketball players. Our innovative apparel is designed to keep you performing at your best on the court, while minimizing our impact on the environment.\n",
      "\n",
      "**What sets us apart:**\n",
      "\n",
      "* **Sustainable Materials**: Our jerseys and shorts are made from a unique blend of recycled polyester, organic cotton, and Tencel, reducing waste and minimizing carbon footprint.\n",
      "* **Moisture-wicking Technology**: Our fabric is designed to keep you cool and dry during even the most intense games, ensuring maximum comfort and performance.\n",
      "* **Breathable Mesh Panels**: Strategically placed mesh panels provide ventilation and flexibility, allowing for a full range of motion on the court.\n",
      "\n",
      "**Features:**\n",
      "\n",
      "* **Quick-drying and moisture-wicking properties**\n",
      "* **Four-way stretch for ultimate mobility**\n",
      "* **Anti-odor technology to keep you fresh all game long**\n",
      "* **Reflective accents for increased visibility during night games or practices**\n",
      "\n",
      "**Join the EcoHoops Movement:**\n",
      "\n",
      "At EcoHoops, we're passionate about creating a more sustainable future in sports. By choosing our eco-friendly sportswear, you'll not only be performing at your best on the court, but also contributing to a reduced environmental impact.\n",
      "\n",
      "**Shop with us today and experience the difference for yourself!**\n",
      "\n",
      "Order now and get 15% off your first purchase with code: ECOHOOPS15\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product}], \n",
    "    options={\"temperature\": 0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now, let's pass the token limit. In Ollama, we use the `\"num_predict\"` option. I'll set it to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a product description for EcoHoops:\n",
      "\n",
      "**Introducing EcoHoops: The Sustainable Game-Changer in Basketball Sportswear**\n",
      "\n",
      "Take your game to the next level while doing good for the planet with EcoHoops, the ultimate eco-friendly\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product}], \n",
    "    options={\"num_predict\": 50, \"temperature\": 0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the problem?\n",
    "\n",
    "The model generates the same poem description with a hard-stop when reaching the token limit.\n",
    "\n",
    "So the response is incomplete.\n",
    "\n",
    "By using max tokens, we risk that the model will not finish its response.\n",
    "\n",
    "<img src=\"images/tokens.png\" alt=\"systemImage\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if I actually wanted a shorter description, I'd say it in the prompts, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"EcoHoops is the game-changing, eco-friendly sportswear for ballers. Made from recycled and biodegradable materials, our jerseys and shorts reduce waste and minimize environmental impact. Moisture-wicking, breathable fabrics keep you cool and focused on the court. Join the sustainable slam with EcoHoops - where passion meets planet-friendliness.\"\n"
     ]
    }
   ],
   "source": [
    "prompt_product_short = \"Create a 50-word product description for EcoHoops - an eco-friendly sportswear for basketball players\"\n",
    "\n",
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using max tokens is practical and widely used. But be careful when using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "A nice feature of Ollama is the ability to stream responses. Afrer using ChatGPT or Claude, we expect the responses to run as streams. Here's how to do it.\n",
    "\n",
    "The biggest change will come from the `stream` parameter. We just set it to `True`. \n",
    "\n",
    "But we also need to run the `ollama.chat()` in a for loop.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa)."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}]\n",
    "\n",
    "for chunk in ollama.chat(model=model, messages=messages, stream=True):\n",
    "    token = chunk[\"message\"][\"content\"]\n",
    "    if token is not None:\n",
    "        print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Congrats! You just learned a bunch!\n",
    "\n",
    "You now know:\n",
    "- how to run Llama 3 with Ollama\n",
    "- how to stream your responses using Ollama\n",
    "- the importance and usage of the system prompt\n",
    "- the meaning and practical applications of the LLMs temperature\n",
    "- how to reproduce responses using the seed parameter\n",
    "- the pros and cons of using the max tokens option\n",
    "\n",
    "Now take the code and try play with it!\n",
    "\n",
    "Play with the prompts and options and see how the results change.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
