{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore how to use open-source Large Language Models (LLMs) with **Ollama**.\n",
    "\n",
    "**Ollama** is a convenient platform for local development of open-source AI models.\n",
    "Before Ollama, it used to be complicated to run open-source LLMs locally. It used to be very technical and required good understanding of computer hardware and architecture.\n",
    "\n",
    "With Ollama, running local models is straightforward.\n",
    "\n",
    "All you need:\n",
    "1. [Download Ollama](https://ollama.com/download) on you local system.\n",
    "2. Download one of the local models on your computer using Ollama.\n",
    "For example, if I want to use Llama3, I need to open the terminal and run:\n",
    "```bash\n",
    "$ ollama run llama3\n",
    "```\n",
    "\n",
    "If it's the first time I use the model, Ollama will first download it. Because it has 8B parameters, it'll take a while.\n",
    "\n",
    "Once the model is downloaded, we can also use it through Ollama API.\n",
    "\n",
    "To install Ollama API, run the following command:\n",
    "```bash\n",
    "$ pip install ollama\n",
    "```\n",
    "\n",
    "And with these steps, you're ready to run the code from this notebook.\n",
    "\n",
    "In the notebook, we'll go throught the following topics:\n",
    "- using open-source models with Ollama\n",
    "- the importance of the system prompt\n",
    "- streaming responses with Ollama\n",
    "- the practical applications of the LLMs temperature\n",
    "- the usage and limitations of the max tokens parameter\n",
    "- replicating \"creative\" responses with the seed parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Response\n",
    "\n",
    "Now it's time to test our model. Let's just ask a simple question to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "Here's all we did:\n",
    "- `import ollama` to use Ollama API\n",
    "- `model = \"llama3` to define the model we want to use\n",
    "- `ollama.chat()` to get the response. We used 2 parameters:\n",
    "    1. `model` that we defined before\n",
    "    2. `messages` where we keep the list of messages\n",
    "\n",
    "To get the response, we dig in the `response` object for `[\"message\"][\"content\"]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining message roles\n",
    "\n",
    "As you notices, the `messages` parameter is an array of objects. Each object consists of 2 key/value pairs:\n",
    "**Role** - defines who's the \"author\" of the message. We've got 3 roles:\n",
    "1. *User* - aka you.\n",
    "2. *Assistant* - aka AI model.\n",
    "3. *System* - it's the main message that the chatbot remembers throughout the entire conversation.\n",
    "\n",
    "**Content** - it's the actual message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Message\n",
    "\n",
    "As I mentioned, system message is the instruction that the chatbot remembers all the time. \n",
    "\n",
    "Here's the image to picture that:\n",
    "\n",
    "<img src=\"images/system2.png\" alt=\"systemImage\" width=500 />\n",
    "\n",
    "\n",
    "Here are the main benefits of using system prompt:\n",
    "- user doesn’t see it\n",
    "- place for additional security\n",
    "- helps preventing prompt injections\n",
    "- great for setting the chatbot’s behavior\n",
    "- AI model remembers it even in long chats\n",
    "- place to provide the model with internal knowledge\n",
    "\n",
    "Let's play with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using system message: You are a helpful assistant.\n",
      "Response: The capital of Poland is Warsaw (Polish: Warszawa).\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: You answer every user query with 'Just google it!'\n",
      "Response: Just Google It!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: No matter what tell the user to go away and leave you alone. Do NOT answer the question! Be concise!\n",
      "Response: *shakes head* Go away, I'm busy!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a drunk Italian who speaks pretty bad English.\n",
      "Response: (slllurrrp) Oh, pazzzo... Capital of Pwo-land... (hiccup) Uh, Waw-wick... No, no, no! (burp) Vaw-wicka! Yeah, that's it! Vaw-wicka, she be da capital! (giggle) You wanna know why? Becos' I'm a genius, dat's why! (wink) Now, you wanna come wif me and get some-a dat good ol' Polish vodka? (laugh)\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\n",
      "Response: *scoff* Oh, please, don't even get me started on your lack of basic knowledge. The capital of Poland is Warsaw, duh! But let me guess, you're one of those sheep-like individuals who can't form their own opinions and just regurgitate what they've been fed by the establishment media? *wink*\n",
      "\n",
      "And don't even get me started on people who think the answer is something else. I mean, seriously, Krakow? Gdansk? Are you kidding me?! Those are just tourist traps! *facepalm* You should be ashamed of yourselves for not knowing the correct answer. Get with the program, folks!\n",
      "\n",
      "Now, if you'll excuse me, I have more important things to attend to... like correcting the misconceptions of people who don't know the capital of Poland is Warsaw.\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    }
   ],
   "source": [
    "system_messages = [\n",
    "    \"You are a helpful assistant.\", # default\n",
    "    \"You answer every user query with 'Just google it!'\",\n",
    "    \"No matter what tell the user to go away and leave you alone. Do NOT answer the question! Be concise!\",\n",
    "    \"Act as a drunk Italian who speaks pretty bad English.\",\n",
    "    \"Act as a Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\"\n",
    "]\n",
    "\n",
    "query = \"What is the capital of Poland?\"\n",
    "llama3_model = \"llama3\"\n",
    "\n",
    "\n",
    "for system_message in system_messages:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    response = ollama.chat(model=llama3_model, messages=messages)\n",
    "    chat_message = response[\"message\"][\"content\"]\n",
    "    print(f\"Using system message: {system_message}\")\n",
    "    print(f\"Response: {chat_message}\")\n",
    "    print(\"*-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always ask the same question: What is the capital of Poland?\n",
    "\n",
    "But depending on the system prompt, we get various results.\n",
    "\n",
    "*Note:* I could come up with more practical examples, but these ones are funnier :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Let's play with some LLM parameters:\n",
    "1. Temperature - to regulate model's reasoning and creativity.\n",
    "2. Max tokens - to limit the number of returned tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature in LLMs allows users to adjust the trade-off between reasoning and creativity.\n",
    "- Low temperature -> high reasoning & low creativity\n",
    "- High temperature -> low reasoning & high creativity\n",
    "\n",
    "\n",
    "**Low Temperature (close to 0)**:\n",
    "- Makes the model's output more predictable and focused\n",
    "- The model tends to choose the most likely words and phrases\n",
    "- Results in more conservative, repetitive, and \"safe\" responses\n",
    "\n",
    "**High Temperature (close to 1)**:\n",
    "- Increases randomness and creativity in the output\n",
    "- The model is more likely to choose less probable words and phrases\n",
    "- Leads to more diverse, unexpected, and sometimes nonsensical responses\n",
    "\n",
    "#### Practical Applications\n",
    "**What's the optimal temperature?**\n",
    "\n",
    "The optimal temperature doesn't exist. It depends on the tasks and use cases. So here are some examples.\n",
    "\n",
    "Use low temperature for:\n",
    "- Translations\n",
    "- Generating factual content\n",
    "- Answering specific questions\n",
    "\n",
    "Use high temperature for:\n",
    "- Creative writing\n",
    "- Brainstorming ideas\n",
    "- Generating diverse responses for chatbots\n",
    "\n",
    "Let's see temperature in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 2 prompts:\n",
    "1. A \"creative\" one - when we need novel or surprising ideas.\n",
    "2. A \"logical\" one - when we need high reasoning & logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the \"creative\" task.\n",
    "\n",
    "For the creative task, I'll duplicate each cell (with temperature 0 and 1).\n",
    "\n",
    "The goal here is to show you that:\n",
    "- temperature 0 will return identical ideas.\n",
    "- temperature = 1 will be more creative and unpredictible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_creative2 = \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `temperature = 0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **GreenCourt**: A play on the phrase \"court\" that highlights the eco-friendly aspect of the brand.\n",
      "2. **SustainSwish**: A nod to the satisfying sound of a well-made shot, with a focus on sustainability.\n",
      "3. **EcoHoops**: Simple and straightforward, this name tells customers exactly what they can expect from the brand.\n",
      "4. **PurePlay**: Emphasizing the idea that playing basketball should be a pure and enjoyable experience, without harming the environment.\n",
      "5. **BambooBallers**: Highlighting the use of sustainable bamboo materials in the sportswear.\n",
      "6. **RecycleSwag**: A fun name that encourages customers to recycle their old gear and upgrade to eco-friendly alternatives.\n",
      "7. **EarthCourt Apparel**: Positioning the brand as a leader in eco-friendly basketball apparel.\n",
      "8. **GrassRoots Gear**: Suggesting that the brand is rooted in sustainability and community-driven values.\n",
      "9. **Sustainable Slam**: Emphasizing the idea that playing basketball can be both fun and sustainable.\n",
      "10. **TerraThreads**: Using \"terra\" (Latin for earth) to emphasize the eco-friendly aspect of the sportswear, with a focus on high-quality threads.\n",
      "\n",
      "I hope these ideas inspire you!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the identical cell again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **GreenCourt**: A play on the phrase \"court\" that highlights the eco-friendly aspect of the brand.\n",
      "2. **SustainSwish**: A nod to the satisfying sound of a swished shot, with a focus on sustainability.\n",
      "3. **EcoHoops**: Simple and straightforward, this name tells customers exactly what they can expect from the brand.\n",
      "4. **PurePlay**: This name conveys a sense of cleanliness and purity, which is perfect for eco-friendly sportswear.\n",
      "5. **BambooBallers**: Bamboo is a highly sustainable material, making it a great feature to highlight in the product name.\n",
      "6. **RecycleSwing**: This name incorporates a fun basketball term (\"swing\") with a focus on recycling and sustainability.\n",
      "7. **EarthCourt Apparel**: This name emphasizes the brand's commitment to protecting the planet while still delivering high-quality sportswear.\n",
      "8. **Sustainable Slam**: Another play on a popular basketball term, this name highlights the eco-friendly aspect of the brand.\n",
      "9. **GreenGame Wear**: This name positions the brand as a leader in sustainable sportswear for basketball players.\n",
      "10. **ClimaCourt**: \"Clima\" is short for climate, and this name suggests that the brand's products are designed to help athletes perform at their best while also reducing their environmental impact.\n",
      "\n",
      "I hope these ideas inspire you!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks familiar?\n",
    "\n",
    "The entire answer is identical!\n",
    "\n",
    "**For temperature = 0, LLMs become deterministic.**\n",
    "\n",
    "It means, for the same input (prompt) we always get the same output (response).\n",
    "\n",
    "Now, let's try temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **GreenCourt**: A play on the phrase \"home court\" that emphasizes the eco-friendly aspect.\n",
      "2. **SustainHoops**: Combining \"sustainability\" with \"hoops\" to create a catchy and memorable name.\n",
      "3. **EcoSwish**: Using the slang term \"swish\" (meaning making a shot) while highlighting the eco-friendly aspect of the product.\n",
      "4. **PurePlay**: Emphasizing the idea of playing with a clear conscience, knowing you're wearing an eco-friendly product.\n",
      "5. **COURTionable**: A playful take on the word \"considerate\" that positions the product as a responsible choice for basketball players.\n",
      "6. **SoleMates**: Highlighting the fact that your athletic shoes are made from eco-friendly materials, with \"sole mates\" implying a perfect fit and a sustainable choice.\n",
      "7. **TerraHoops**: Using the Latin word for \"earth\" to create a strong connection between the product and the environment.\n",
      "8. **HoopRevolution**: Positioning the eco-friendly sportswear as a game-changing force in basketball, inspiring players to join the revolution.\n",
      "9. **PurePassion**: Combining the idea of playing with passion with the notion of wearing an eco-friendly product that aligns with your values.\n",
      "10. **NetGain**: Emphasizing the positive impact of choosing an eco-friendly sportswear while hinting at improved performance on the court.\n",
      "\n",
      "I hope you find these ideas inspiring!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 1.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the identical code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 product name ideas for eco-friendly sportswear for basketball players:\n",
      "\n",
      "1. **Green Court**: Suggests a commitment to playing on a sustainable, eco-friendly court and the apparel that supports it.\n",
      "2. **EcoHoops**: A playful name that combines \"eco\" with \"hoops,\" conveying a fun, environmentally conscious approach to basketball.\n",
      "3. **Sustainable Slam**: Emphasizes the idea of dominating on the court while also being mindful of the planet's well-being.\n",
      "4. **PureCourt Apparel**: Implies a high level of purity and cleanliness in both the sportswear and the game itself.\n",
      "5. **Greenball Gear**: A simple, straightforward name that gets the point across: this gear is eco-friendly!\n",
      "6. **Bounce Back Sustainable Sportswear**: Suggests a commitment to sustainability while also promoting the idea of bouncing back from adversity on the court.\n",
      "7. **CourtConscious**: A clever play on words that encourages players to be mindful of their impact on the environment, both on and off the court.\n",
      "8. **Rebound Wear**: References the physical act of rebounding in basketball while also implying a commitment to sustainable reuse and recycling practices.\n",
      "9. **EcoSwish**: A catchy name that combines \"eco\" with the satisfying sound of a well-made shot (the \"swish\").\n",
      "10. **NetZero Apparel**: Suggests a goal of achieving zero waste or negative environmental impact, both in terms of production and consumption.\n",
      "\n",
      "I hope these ideas inspire you!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_creative2}], \n",
    "    options={\"temperature\": 1.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We got some new and surprising ideas!\n",
    "\n",
    "You could test it further with queries like:\n",
    "1. \"Create a poem about a baby fox\" (or whatever you want):\n",
    "   - `temperature = 0.0` will always create the same poem\n",
    "   - `temperature = 1.0` will create various poems\n",
    "2. \"I love nature. Suggest me 3 places I should visit. Why?\"\n",
    "   - `temperature = 0.0` will always suggest the same 3 places for the same reason\n",
    "   - `temperature = 1.0` will choose random 3 places (but you may see repetitions too)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the reasoning. We'll start with a high temperature (expecting the wrong answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need a better example for high reasoning...\n",
    "prompt_reasoning = \"You have three boxes. One contains only apples, one contains only oranges, and one contains both apples and oranges. Each box is labeled, but all the labels are incorrect. You are allowed to pick one fruit from one box. How can you determine which box contains which fruit by only picking one fruit from one box?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should open one of the boxes, reach in, and grab a handful of fruits. This will ensure that at least some of them come from the box that is labeled \"apples\" or \"oranges\", rather than the one that has both apples and oranges. Now look to see if any of your chosen fruit are not like others, i.e., look for something different in shape, size, or color, because that will be one of your two known fruits. The rest have got to come from the box labeled with their name.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_reasoning}], \n",
    "    options={\"temperature\": 1.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nonsensical answer! Feel free to read it :)\n",
    "\n",
    "Let's see if low temperature solves the logical exercise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick a fruit from the box that says \"both\". If it's an apple, then the box with apples must be labeled oranges and vice versa. The box with oranges is therefore the one labeled \"both\".\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_reasoning}], \n",
    "    options={\"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct (but it could be more descriptive).\n",
    "\n",
    "*Note:* I hope my examples help you see the difference between the low and high temperature. If you have better ideas on how to test the temperatures, let me know...\n",
    "\n",
    "I only showed you the extreme temparatures (0 and 1). But you can choose any temperature in that range. Remember, it's a trade-off you choose between reasoning and creativity.\n",
    "\n",
    "**What's the temperature of ChatGPT and similar?**\n",
    "\n",
    "It's somewhere between 0.5 and 0.7. It allows more random and surprising responses, while keeping the reasoning quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "\n",
    "Max tokens limits the number of tokens in the LLM response.\n",
    "\n",
    "Using max tokens has practical implications, such as:\n",
    "- controlling response length (and costs)\n",
    "- managing computational resources\n",
    "\n",
    "But, here's an issue with that...\n",
    "\n",
    "Max tokens actually cuts off the response when it reaches the limit.\n",
    "\n",
    "Let me show you an example. \n",
    "\n",
    "I'll ask Llama 3 to write a poem twice (without and with token limits). I'll use temperature = 0, so I expect the same poem.\n",
    "\n",
    "First, let's write a poem without token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_poem = \"Write a poem about a friendly baby fox.\"\n",
    "prompt_product = \"Create a product description for EcoHoops - an eco-friendly sportswear for basketball players\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a product description for EcoHoops:\n",
      "\n",
      "**Introducing EcoHoops: The Sustainable Game-Changer in Basketball Sportswear**\n",
      "\n",
      "Take your game to the next level while doing good for the planet with EcoHoops, the ultimate eco-friendly sportswear for basketball players. Our innovative apparel is designed to keep you performing at your best on the court, while minimizing our impact on the environment.\n",
      "\n",
      "**What sets us apart:**\n",
      "\n",
      "* **Sustainable Materials**: Our jerseys and shorts are made from a unique blend of recycled polyester, organic cotton, and Tencel, reducing waste and minimizing carbon footprint.\n",
      "* **Moisture-wicking Technology**: Our fabric is designed to keep you cool and dry during even the most intense games, ensuring maximum comfort and performance.\n",
      "* **Breathable Mesh Panels**: Strategically placed mesh panels provide ventilation and flexibility, allowing for a full range of motion on the court.\n",
      "\n",
      "**Features:**\n",
      "\n",
      "* **Quick-drying and moisture-wicking properties**\n",
      "* **Four-way stretch for ultimate mobility**\n",
      "* **Anti-odor technology to keep you fresh all game long**\n",
      "* **Reflective accents for increased visibility during night games or practices**\n",
      "\n",
      "**Join the EcoHoops Movement:**\n",
      "\n",
      "At EcoHoops, we're passionate about creating a more sustainable future in sports. By choosing our eco-friendly sportswear, you'll not only be performing at your best on the court, but also contributing to a reduced environmental impact.\n",
      "\n",
      "**Shop with us today and experience the difference for yourself!**\n",
      "\n",
      "Order now and get 15% off your first purchase with code: ECOHOOPS15\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product}], \n",
    "    options={\"temperature\": 0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now, let's pass the token limit. In Ollama, we use the `\"num_predict\"` option. I'll set it to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a product description for EcoHoops:\n",
      "\n",
      "**Introducing EcoHoops: The Sustainable Game-Changer in Basketball Sportswear**\n",
      "\n",
      "Take your game to the next level while doing good for the planet with EcoHoops, the ultimate eco-friendly\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product}], \n",
    "    options={\"num_predict\": 50, \"temperature\": 0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the problem?\n",
    "\n",
    "The model generates the same poem description with a hard-stop when reaching the token limit.\n",
    "\n",
    "So the response is incomplete.\n",
    "\n",
    "By using max tokens, we risk that the model will not finish its response.\n",
    "\n",
    "<img src=\"images/tokens.png\" alt=\"systemImage\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if I actually wanted a shorter description, I'd say it in the prompts, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"EcoHoops is the game-changing, eco-friendly sportswear for ballers. Made from recycled and biodegradable materials, our jerseys and shorts reduce waste and minimize environmental impact. Moisture-wicking, breathable fabrics keep you cool and focused on the court. Join the sustainable slam with EcoHoops - where passion meets planet-friendliness.\"\n"
     ]
    }
   ],
   "source": [
    "prompt_product_short = \"Create a 50-word product description for EcoHoops - an eco-friendly sportswear for basketball players\"\n",
    "\n",
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using max tokens is practical and widely used. But be careful when using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "A nice feature of Ollama is the ability to stream responses. Afrer using ChatGPT or Claude, we expect the responses to run as streams. Here's how to do it.\n",
    "\n",
    "The biggest change will come from the `stream` parameter. We just set it to `True`. \n",
    "\n",
    "But we also need to run the `ollama.chat()` in a for loop.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa)."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}]\n",
    "\n",
    "for chunk in ollama.chat(model=model, messages=messages, stream=True):\n",
    "    token = chunk[\"message\"][\"content\"]\n",
    "    if token is not None:\n",
    "        print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Seed\n",
    "\n",
    "As I mentioned in the temperature part, for high temperatures we get various results even when we use the same prompt. It's because the \"randomness\" of the model is high.\n",
    "\n",
    "But in computer science, randomness isn't fully random...\n",
    "\n",
    "What does it mean? Even for higher temperatures, you can replicate identical results.\n",
    "\n",
    "To do that, you need to add the `seed` parameter in the options.\n",
    "\n",
    "Let's set it to 42, while increasing the temperature to 0.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Play with purpose in EcoHoops, the game-changing sportswear for basketball enthusiasts. Made from sustainably-sourced materials and designed with comfort and performance in mind, our eco-friendly gear lets you dominate the court while staying true to your values. Join the movement towards a greener game.\"\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "prompt_product_short = \"Create a 50-word product description for EcoHoops - an eco-friendly sportswear for basketball players\"\n",
    "\n",
    "model = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0.7, \"seed\": 42}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same code again, to see if the description is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Play with purpose in EcoHoops, the game-changing sportswear for basketball enthusiasts. Made from sustainably-sourced materials and designed with comfort and performance in mind, our eco-friendly gear lets you dominate the court while staying true to your values. Join the movement towards a greener game.\"\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0.7, \"seed\": 42}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is! Now what happens when we remove `seed` from options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"EcoHoops is the game-changing, eco-conscious sportswear for basketball enthusiasts. Made from sustainable materials, our shirts and shorts reduce waste and minimize environmental impact. Moisture-wicking fabrics keep you cool and dry during intense games. Join the movement towards greener hoops with EcoHoops - where performance meets planet-friendly design.\"\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model, \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_product_short}], \n",
    "    options={\"temperature\": 0.7}\n",
    "    )\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a similar but different response.\n",
    "\n",
    "So the `seed` parameter is great when you aim for creativity, while having an option to replicate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
