{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore how to use open-source models with **Ollama**.\n",
    "\n",
    "**Ollama** is a convenient platform for local development of open-source AI models.\n",
    "Before Ollama, it used to be complicated to run open-source LLMs locally. It used to be very technical and required good understanding of computer hardware and architecture.\n",
    "\n",
    "With Ollama, running local models is straightforward.\n",
    "\n",
    "Here's all you need:\n",
    "1. [Download Ollama](https://ollama.com/download) on you local system.\n",
    "2. Download one of the local models on your computer using Ollama.\n",
    "For example, if I want to use Llama3.1 (the newest Llama model at the time of writing), I need to open the terminal and run:\n",
    "```bash\n",
    "$ ollama run llama3.1\n",
    "```\n",
    "\n",
    "If it's the first time I use the model, Ollama will first download it. Because it has 8B parameters, it'll take a while.\n",
    "\n",
    "Once the model is downloaded, we can also use it through Ollama API.\n",
    "\n",
    "To install Ollama API, run the following command:\n",
    "```bash\n",
    "$ pip install ollama\n",
    "```\n",
    "\n",
    "And with these steps, you're ready to run the code from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Response\n",
    "\n",
    "Now it's time to test our model. Let's just ask a simple question to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw (Polish: Warszawa).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "model = \"llama3\"\n",
    "\n",
    "response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": \"What's the capital of Poland?\"}])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "Here's all we did:\n",
    "- `import ollama` to use Ollama API\n",
    "- `model = \"llama3` to define the model we want to use\n",
    "- `ollama.chat()` to get the response. We used 2 parameters:\n",
    "    1. `model` that we defined before\n",
    "    2. `messages` where we keep the list of messages\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
